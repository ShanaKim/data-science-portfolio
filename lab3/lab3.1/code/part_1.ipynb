{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "from preprocessing import make_delayed\n",
    "from preprocessing import downsample_word_vectors\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "# Add the root project folder to sys.path (so ridge_utils becomes importable)\n",
    "project_root = os.path.abspath('..')  # moves up from 'code/'\n",
    "sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "109\n"
     ]
    }
   ],
   "source": [
    "# Load the raw_text.pkl file\n",
    "path_to_data = '/ocean/projects/mth240012p/shared/data'\n",
    "with open(f'{path_to_data}/raw_text.pkl', 'rb') as f:\n",
    "    raw_text = pickle.load(f)\n",
    "\n",
    "print(type(raw_text))\n",
    "print(len(raw_text)) # total 109 stories\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before splitting the stories into train and test, since there are 8 more stories in the raw_text file than in the stories in subject 2 and 3, we exclude these stories since they cannot be used for both test or train. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "101\n"
     ]
    }
   ],
   "source": [
    "all_stories = set(raw_text.keys())\n",
    "subj2_stories = set(os.path.splitext(f)[0] for f in os.listdir(f'{path_to_data}/subject2') if f.endswith('.npy'))\n",
    "subj3_stories = set(os.path.splitext(f)[0] for f in os.listdir(f'{path_to_data}/subject3') if f.endswith('.npy'))\n",
    "print(subj2_stories == subj3_stories) #fortunately, subject 2 and 3 has same stories\n",
    "valid_stories = sorted(list(all_stories & subj2_stories & subj3_stories))\n",
    "print(len(valid_stories)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we split test stories, train stories (3:7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train stories: 70\n",
      "Test stories: 31\n"
     ]
    }
   ],
   "source": [
    "#we will just rename valid_stories as all_stories for the sake of simplicity.\n",
    "all_stories = valid_stories\n",
    "random.seed(42)\n",
    "random.shuffle(all_stories)\n",
    "split_idx = int(0.7 * len(all_stories))  # 70% for training\n",
    "\n",
    "train_stories = all_stories[:split_idx]\n",
    "test_stories = all_stories[split_idx:]\n",
    "\n",
    "print(f\"Train stories: {len(train_stories)}\")\n",
    "print(f\"Test stories: {len(test_stories)}\")\n",
    "train_stories.sort()\n",
    "test_stories.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bag of Words Method**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. For each story in train, we compute a matrix of one-hot vectors for each word in story.\n",
    "\n",
    "Below, we first get a list of all the unique words in all the stories in train_stories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137105\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10360"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#compute and append all words in train_stories\n",
    "allwords = []\n",
    "for story in train_stories:\n",
    "    temp_text = raw_text[story].data\n",
    "    allwords += temp_text\n",
    "\n",
    "print(len(allwords))\n",
    "\n",
    "#compute unique words in train_stories\n",
    "unique_words = list(set(allwords))\n",
    "len(unique_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137105\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10360"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#compute and append all words in train_stories\n",
    "allwords = []\n",
    "for story in train_stories:\n",
    "    temp_text = raw_text[story].data\n",
    "    allwords += temp_text\n",
    "\n",
    "print(len(allwords))\n",
    "\n",
    "#compute unique words in train_stories\n",
    "unique_words = list(set(allwords))\n",
    "len(unique_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make one-hot matrix for each story in training stories\n",
    "\n",
    "word_to_index = {word: idx for idx, word in enumerate(unique_words)} #match each word to index\n",
    "vocab_size = len(unique_words)\n",
    "\n",
    "onehot_matrices = {}  # Storing one-hot matrix per story\n",
    "\n",
    "for story in train_stories:\n",
    "    words = raw_text[story].data\n",
    "    num_words = len(words)\n",
    "    \n",
    "    # Initialize zero matrix\n",
    "    onehot = np.zeros((num_words, vocab_size), dtype=np.int8)\n",
    "    \n",
    "    # Fill in the one-hot vectors\n",
    "    for i, word in enumerate(words):\n",
    "        if word in word_to_index:\n",
    "            j = word_to_index[word]\n",
    "            onehot[i, j] = 1\n",
    "    \n",
    "    onehot_matrices[story] = onehot #store matrix as value and story name as key\n",
    "\n",
    "len(onehot_matrices) #70 keys == 70 train stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First key: adventuresinsayingyes\n",
      "<class 'numpy.ndarray'>\n",
      "original shape of matrix for first story is: (2309, 10360)\n",
      "total length of words in first story is: 1708\n"
     ]
    }
   ],
   "source": [
    "#check if one-hot matrices are calculated well for first training story\n",
    "for key, value in onehot_matrices.items():\n",
    "    print(\"First key:\", key)\n",
    "    print(type(value))\n",
    "    print(\"original shape of matrix for first story is:\", value.shape) #row:number of total words in story\n",
    "    #column: number of unique words in all training stories\n",
    "    break\n",
    "\n",
    "\n",
    "print(\"total length of words in first story is:\", len(raw_text['itsabox'].data)) \n",
    "#number of rows of matrix matches the total length of words in first story"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Downsampling word vectors (one-hot matrices) to number of FMRIs taken for each story\n",
    "Now we call downsample word vectors from the preprocessing.py to get the dimensions to match. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#made dictionary whose keys are name of train story and values are raw_text[story]\n",
    "wordseqs = {story: raw_text[story] for story in train_stories}\n",
    "\n",
    "downsampled_vectors_1 = downsample_word_vectors(\n",
    "    stories=train_stories,\n",
    "    word_vectors=onehot_matrices,\n",
    "    wordseqs=wordseqs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n",
      "First key: adventuresinsayingyes\n",
      "downsampled matrix for first story is (406, 10360)\n",
      "First key: afatherscover\n",
      "downsampled matrix for first story is (327, 10360)\n"
     ]
    }
   ],
   "source": [
    "print(len(downsampled_vectors_1)) #matches the total number of training stories\n",
    "\n",
    "#check the downsampled matrix of first two stories, 'adollshouse' and 'adventuresinsayingyes'\n",
    "i=0\n",
    "for key, value in downsampled_vectors_1.items():\n",
    "    print(\"First key:\", key)\n",
    "    print(\"downsampled matrix for first story is\", value.shape)\n",
    "    i += 1\n",
    "    if i == 2: break\n",
    "#we can see that row of the matrix shrunk from 1708 to 370"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First key: adventuresinsayingyes\n",
      "trimmed matrix for first story is (391, 10360)\n"
     ]
    }
   ],
   "source": [
    "trimmed_vectors_1 = {} #trimmed matrices for each train story \n",
    "\n",
    "for story, matrix in downsampled_vectors_1.items():\n",
    "    trimmed_matrix = matrix[5:-10]\n",
    "    trimmed_vectors_1[story] = trimmed_matrix\n",
    "\n",
    "# check for first trimmed matrix, which corresponds to story \n",
    "for key, value in trimmed_vectors_1.items():\n",
    "    print(\"First key:\", key)\n",
    "    print(\"trimmed matrix for first story is\", value.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: <class 'numpy.ndarray'>\n",
      "Shape: (241, 94251)\n"
     ]
    }
   ],
   "source": [
    "#We can see that the number of rows of trimmed matrices matches\n",
    "# the number of rows in the FMRI scan for 'adollshouse' story\n",
    "itsabox_s2 = np.load(f'{path_to_data}/subject2/adollshouse.npy')\n",
    "\n",
    "print(\"Type:\", type(itsabox_s2))\n",
    "print(\"Shape:\", itsabox_s2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Create lagged versions of the features using make_delayed with delays ranging form [1, 4] inclusive\n",
    "\n",
    "Because the BOLD response lags behind the stimulus. If a word is spoken at time t, the associated brain response might show up 4–6 seconds later (2–3 TRs later).\n",
    "So we remake each row vector for each TR so that the row vector for that particular TR\n",
    "includes the information of the previous 4 TRs. \n",
    "\n",
    "Hence, for each matrix, the number of columns will increase by 4 folds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First key: adventuresinsayingyes\n",
      "delayed matrix for first story is (391, 41440)\n"
     ]
    }
   ],
   "source": [
    "delayed_vectors_1 = {}\n",
    "\n",
    "for story, trimmed_matrix in trimmed_vectors_1.items():\n",
    "    X_lagged = make_delayed(trimmed_matrix, delays=[1, 2, 3, 4])\n",
    "    delayed_vectors_1[story] = X_lagged\n",
    "\n",
    "# check for first trimmed matrix, which corresponds to story 'penpal'\n",
    "for key, value in delayed_vectors_1.items():\n",
    "    print(\"First key:\", key)\n",
    "    print(\"delayed matrix for first story is\", value.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row now contains the features from the previous 1–4 TRs.\n",
    "This is using the four previous TR's chunk to predict the current TR. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we make delayed vectors for test stories as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(unique_words) #unique words are from TRAIN STORIES\n",
    "\n",
    "onehot_matrices_test = {}  # Storing one-hot matrix per story\n",
    "\n",
    "for story in test_stories:\n",
    "    words = raw_text[story].data\n",
    "    num_words = len(words)\n",
    "    \n",
    "    # Initialize zero matrix\n",
    "    onehot = np.zeros((num_words, vocab_size), dtype=np.int8)\n",
    "    \n",
    "    # Fill in the one-hot vectors\n",
    "    for i, word in enumerate(words):\n",
    "        if word in word_to_index:\n",
    "            j = word_to_index[word]\n",
    "            onehot[i, j] = 1\n",
    "    \n",
    "    onehot_matrices_test[story] = onehot #store matrix as value and story name as key\n",
    "\n",
    "print(len(onehot_matrices_test)) #30 keys\n",
    "\n",
    "wordseqs2 = {story: raw_text[story] for story in test_stories} #wordseqs for test stories\n",
    "downsampled_vectors_1_test = downsample_word_vectors(\n",
    "    stories=test_stories,\n",
    "    word_vectors=onehot_matrices_test,\n",
    "    wordseqs=wordseqs2\n",
    ")\n",
    "\n",
    "trimmed_vectors_1_test = {} #trimmed matrices for each test story \n",
    "\n",
    "for story, matrix in downsampled_vectors_1_test.items():\n",
    "    trimmed_matrix = matrix[5:-10]\n",
    "    trimmed_vectors_1_test[story] = trimmed_matrix\n",
    "\n",
    "delayed_vectors_1_test = {}\n",
    "\n",
    "for story, trimmed_matrix in trimmed_vectors_1_test.items():\n",
    "    X_lagged = make_delayed(trimmed_matrix, delays=[1, 2, 3, 4])\n",
    "    delayed_vectors_1_test[story] = X_lagged\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row now contains the features from the previous 1–4 TRs.\n",
    "This is using the four previous TR's chunk to predict the current TR. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word2Vec method**\n",
    "Now we do the same process for Word2Vec method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Make vectors for each word in train_stories\n",
    "Create matrices for each stories. \n",
    "\n",
    "We use w2v pretrained model that gives a vector of length 300 for each word in train stories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained Google News Word2Vec (300-dim)\n",
    "# You need to download this separately\n",
    "# Download from: https://code.google.com/archive/p/word2vec/\n",
    "\n",
    "#unzip the gz file and put the bin file INSIDE the DATA folder\n",
    "model_path = '../data/GoogleNews-vectors-negative300.bin'\n",
    "w2v = KeyedVectors.load_word2vec_format(model_path, binary=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we check if the W2V model includes all the unique words for all the words in train_stories.\n",
    "We use unique_words that we defined earlier. \n",
    "We see below that about 5% of the words are not defined in w2v, but since 5% is negligible, we just put 0 vectors for the words that aren't defined in w2v model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of words Not in w2v model: 0.0535\n"
     ]
    }
   ],
   "source": [
    "in_model = [word for word in unique_words if word in w2v]\n",
    "not_in_model = [word for word in unique_words if word not in w2v]\n",
    "print(f\"Percentage of words Not in w2v model: {len(not_in_model)/len(unique_words):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.06445312, -0.16015625, -0.01208496,  0.13476562, -0.22949219,\n",
       "        0.16210938,  0.3046875 , -0.1796875 , -0.12109375,  0.25390625,\n",
       "       -0.01428223, -0.06396484, -0.08056641, -0.05688477, -0.19628906,\n",
       "        0.2890625 , -0.05151367,  0.14257812, -0.10498047, -0.04736328,\n",
       "       -0.34765625,  0.35742188,  0.265625  ,  0.00188446, -0.01586914,\n",
       "        0.00195312, -0.35546875,  0.22167969,  0.05761719,  0.15917969,\n",
       "        0.08691406, -0.0267334 , -0.04785156,  0.23925781, -0.05981445,\n",
       "        0.0378418 ,  0.17382812, -0.41796875,  0.2890625 ,  0.32617188,\n",
       "        0.02429199, -0.01647949, -0.06494141, -0.08886719,  0.07666016,\n",
       "       -0.15136719,  0.05249023, -0.04199219, -0.05419922,  0.00108337,\n",
       "       -0.20117188,  0.12304688,  0.09228516,  0.10449219, -0.00408936,\n",
       "       -0.04199219,  0.01409912, -0.02111816, -0.13476562, -0.24316406,\n",
       "        0.16015625, -0.06689453, -0.08984375, -0.07177734, -0.00595093,\n",
       "       -0.00482178, -0.00089264, -0.30664062, -0.0625    ,  0.07958984,\n",
       "       -0.00909424, -0.04492188,  0.09960938, -0.33398438, -0.3984375 ,\n",
       "        0.05541992, -0.06689453, -0.04467773,  0.11767578, -0.13964844,\n",
       "       -0.26367188,  0.17480469, -0.17382812, -0.40625   , -0.06738281,\n",
       "       -0.07617188,  0.09423828,  0.20996094, -0.16308594, -0.08691406,\n",
       "       -0.0534668 , -0.10351562, -0.07617188, -0.11083984, -0.03515625,\n",
       "       -0.14941406,  0.0378418 ,  0.38671875,  0.14160156, -0.2890625 ,\n",
       "       -0.16894531, -0.140625  , -0.04174805,  0.22753906,  0.24023438,\n",
       "       -0.01599121, -0.06787109,  0.21875   , -0.42382812, -0.5625    ,\n",
       "       -0.49414062, -0.3359375 ,  0.13378906,  0.01141357,  0.13671875,\n",
       "        0.0324707 ,  0.06835938, -0.27539062, -0.15917969,  0.00121307,\n",
       "        0.01208496, -0.0039978 ,  0.00442505, -0.04541016,  0.08642578,\n",
       "        0.09960938, -0.04296875, -0.11328125,  0.13867188,  0.41796875,\n",
       "       -0.28320312, -0.07373047, -0.11425781,  0.08691406, -0.02148438,\n",
       "        0.328125  , -0.07373047, -0.01348877,  0.17773438, -0.02624512,\n",
       "        0.13378906, -0.11132812, -0.12792969, -0.12792969,  0.18945312,\n",
       "       -0.13867188,  0.29882812, -0.07714844, -0.37695312, -0.10351562,\n",
       "        0.16992188, -0.10742188, -0.29882812,  0.00866699, -0.27734375,\n",
       "       -0.20996094, -0.1796875 , -0.19628906, -0.22167969,  0.08886719,\n",
       "       -0.27734375, -0.13964844,  0.15917969,  0.03637695,  0.03320312,\n",
       "       -0.08105469,  0.25390625, -0.08691406, -0.21289062, -0.18945312,\n",
       "       -0.22363281,  0.06542969, -0.16601562,  0.08837891, -0.359375  ,\n",
       "       -0.09863281,  0.35546875, -0.00741577,  0.19042969,  0.16992188,\n",
       "       -0.06005859, -0.20605469,  0.08105469,  0.12988281, -0.01135254,\n",
       "        0.33203125, -0.08691406,  0.27539062, -0.03271484,  0.12011719,\n",
       "       -0.0625    ,  0.1953125 , -0.10986328, -0.11767578,  0.20996094,\n",
       "        0.19921875,  0.02954102, -0.16015625,  0.00276184, -0.01367188,\n",
       "        0.03442383, -0.19335938,  0.00352478, -0.06542969, -0.05566406,\n",
       "        0.09423828,  0.29296875,  0.04052734, -0.09326172, -0.10107422,\n",
       "       -0.27539062,  0.04394531, -0.07275391,  0.13867188,  0.02380371,\n",
       "        0.13085938,  0.00236511, -0.2265625 ,  0.34765625,  0.13574219,\n",
       "        0.05224609,  0.18164062,  0.0402832 ,  0.23730469, -0.16992188,\n",
       "        0.10058594,  0.03833008,  0.10839844, -0.05615234, -0.00946045,\n",
       "        0.14550781, -0.30078125, -0.32226562,  0.18847656, -0.40234375,\n",
       "       -0.3125    , -0.08007812, -0.26757812,  0.16699219,  0.07324219,\n",
       "        0.06347656,  0.06591797,  0.17285156, -0.17773438,  0.00276184,\n",
       "       -0.05761719, -0.2265625 , -0.19628906,  0.09667969,  0.13769531,\n",
       "       -0.49414062, -0.27929688,  0.12304688, -0.30078125,  0.01293945,\n",
       "       -0.1875    , -0.20898438, -0.1796875 , -0.16015625, -0.03295898,\n",
       "        0.00976562,  0.25390625, -0.25195312,  0.00210571,  0.04296875,\n",
       "        0.01184082, -0.20605469,  0.24804688, -0.203125  , -0.17773438,\n",
       "        0.07275391,  0.04541016,  0.21679688, -0.2109375 ,  0.14550781,\n",
       "       -0.16210938,  0.20410156, -0.19628906, -0.35742188,  0.35742188,\n",
       "       -0.11962891,  0.35742188,  0.10351562,  0.07080078, -0.24707031,\n",
       "       -0.10449219, -0.19238281,  0.1484375 ,  0.00057983,  0.296875  ,\n",
       "       -0.12695312, -0.03979492,  0.13183594, -0.16601562,  0.125     ,\n",
       "        0.05126953, -0.14941406,  0.13671875, -0.02075195,  0.34375   ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v['apple']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define function that returns matrix given word_list, model=w2v, \n",
    "#vector_size = 300 which is from the pretrained model\n",
    "\n",
    "def embed_story_words(word_list, model, vector_size):\n",
    "    embedded = []\n",
    "    for word in word_list:\n",
    "        if word in model:\n",
    "            embedded.append(model[word])\n",
    "        else:\n",
    "            embedded.append(np.zeros(vector_size))  # unknown word\n",
    "    return np.vstack(embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_size = 300 #of pretrained model \n",
    "story_vectors_w2v = {} \n",
    "#Dictionary where key is story name and value is matrix, whose vectors correspond to each word in story\n",
    "\n",
    "for story in train_stories:\n",
    "    words = raw_text[story].data\n",
    "    story_vectors_w2v[story] = embed_story_words(words, w2v, vector_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Downsample and Trim for matrices made by W2V model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we already defines wordseqs in BoW method.\n",
    "downsampled_vectors_2 = downsample_word_vectors(\n",
    "    stories=train_stories,\n",
    "    word_vectors=story_vectors_w2v,\n",
    "    wordseqs=wordseqs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed_vectors_2 = {} #trimmed matrices for each train story \n",
    "\n",
    "for story, matrix in downsampled_vectors_2.items():\n",
    "    trimmed_matrix = matrix[5:-10]\n",
    "    trimmed_vectors_2[story] = trimmed_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Creating lagged versions\n",
    "\n",
    "Since the current number of columns for each matrix ix 300, \n",
    "the new matrices will have 300*4=1200 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First key: adventuresinsayingyes\n",
      "delayed matrix for first story is (391, 1200)\n"
     ]
    }
   ],
   "source": [
    "delayed_vectors_2 = {}\n",
    "\n",
    "for story, trimmed_matrix in trimmed_vectors_2.items():\n",
    "    X_lagged = make_delayed(trimmed_matrix, delays=[1, 2, 3, 4])\n",
    "    delayed_vectors_2[story] = X_lagged\n",
    "\n",
    "# check for first trimmed,delayed matrix, which corresponds to first story\n",
    "for key, value in delayed_vectors_2.items():\n",
    "    print(\"First key:\", key)\n",
    "    print(\"delayed matrix for first story is\", value.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we make embeddings for test stories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n"
     ]
    }
   ],
   "source": [
    "story_vectors_w2v_test = {}\n",
    "for story in test_stories:\n",
    "    words = raw_text[story].data\n",
    "    story_vectors_w2v_test[story] = embed_story_words(words, w2v, vector_size)\n",
    "\n",
    "wordseqs2 = {story: raw_text[story] for story in test_stories}\n",
    "downsampled_vectors_2_test = downsample_word_vectors(\n",
    "    stories=test_stories,\n",
    "    word_vectors=story_vectors_w2v_test,\n",
    "    wordseqs=wordseqs2\n",
    ")\n",
    "\n",
    "trimmed_vectors_2_test = {} #trimmed matrices for each test story \n",
    "for story, matrix in downsampled_vectors_2_test.items():\n",
    "    trimmed_matrix = matrix[5:-10]\n",
    "    trimmed_vectors_2_test[story] = trimmed_matrix\n",
    "\n",
    "delayed_vectors_2_test = {}\n",
    "for story, trimmed_matrix in trimmed_vectors_2_test.items():\n",
    "    X_lagged = make_delayed(trimmed_matrix, delays=[1, 2, 3, 4])\n",
    "    delayed_vectors_2_test[story] = X_lagged\n",
    "\n",
    "print(len(delayed_vectors_2_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "del w2v\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GloVe method** \n",
    "Basically we do the same as the two methods above. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download a pre-trained GloVe file in https://nlp.stanford.edu/projects/glove/\n",
    "#glove.6B.zip <- unzip this file and MOVE the glove.6B.300d.txt to DATA folder\n",
    "\n",
    "#Since glove file is txt file, we will manually make a dictionary, glove_dict\n",
    "#whose keys are words and values are 300 dim. vectors corresponding for each word of Glove Model. \n",
    "glove_file = '../data/glove.6B.300d.txt'\n",
    "\n",
    "glove_dict = {}\n",
    "with open(glove_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = [float(val) for val in values[1:]]\n",
    "        glove_dict[word] = vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use embed_story_words function defined earlier, used in previous model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_size = 300 #of pretrained model (Glove)\n",
    "story_vectors_glv = {} \n",
    "\n",
    "for story in train_stories:\n",
    "    words = raw_text[story].data\n",
    "    story_vectors_glv[story] = embed_story_words(words, glove_dict, vector_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Downsample and trim matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we already defined wordseqs in BoW method.\n",
    "downsampled_vectors_3 = downsample_word_vectors(\n",
    "    stories=train_stories,\n",
    "    word_vectors=story_vectors_glv,\n",
    "    wordseqs=wordseqs\n",
    ")\n",
    "\n",
    "trimmed_vectors_3 = {} #trimmed matrices for each train story \n",
    "\n",
    "for story, matrix in downsampled_vectors_3.items():\n",
    "    trimmed_matrix = matrix[5:-10]\n",
    "    trimmed_vectors_3[story] = trimmed_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Creating lagged version of matrices\n",
    "\n",
    "Same as W2V, 300 * 4 =1200 will be the number of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First key: adventuresinsayingyes\n",
      "delayed matrix for first story is (391, 1200)\n"
     ]
    }
   ],
   "source": [
    "delayed_vectors_3 = {}\n",
    "\n",
    "for story, trimmed_matrix in trimmed_vectors_3.items():\n",
    "    X_lagged = make_delayed(trimmed_matrix, delays=[1, 2, 3, 4])\n",
    "    delayed_vectors_3[story] = X_lagged\n",
    "\n",
    "# check for first trimmed, delayed matrix, which corresponds to story 'itsabox'\n",
    "for key, value in delayed_vectors_3.items():\n",
    "    print(\"First key:\", key)\n",
    "    print(\"delayed matrix for first story is\", value.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n",
      "First key: adollshouse\n",
      "delayed matrix for first story is (241, 1200)\n"
     ]
    }
   ],
   "source": [
    "vector_size = 300 #of pretrained model (Glove)\n",
    "story_vectors_glv_test = {} \n",
    "\n",
    "for story in test_stories:\n",
    "    words = raw_text[story].data\n",
    "    story_vectors_glv_test[story] = embed_story_words(words, glove_dict, vector_size)\n",
    "\n",
    "# we already defined wordseqs2 for test stories. \n",
    "downsampled_vectors_3_test = downsample_word_vectors(\n",
    "    stories=test_stories,\n",
    "    word_vectors=story_vectors_glv_test,\n",
    "    wordseqs=wordseqs2\n",
    ")\n",
    "\n",
    "trimmed_vectors_3_test = {} #trimmed matrices for each test story \n",
    "\n",
    "for story, matrix in downsampled_vectors_3_test.items():\n",
    "    trimmed_matrix = matrix[5:-10]\n",
    "    trimmed_vectors_3_test[story] = trimmed_matrix\n",
    "\n",
    "delayed_vectors_3_test = {}\n",
    "\n",
    "for story, trimmed_matrix in trimmed_vectors_3_test.items():\n",
    "    X_lagged = make_delayed(trimmed_matrix, delays=[1, 2, 3, 4])\n",
    "    delayed_vectors_3_test[story] = X_lagged\n",
    "\n",
    "print(len(delayed_vectors_3_test))\n",
    "#check for first matrix for story\n",
    "for key, value in delayed_vectors_3_test.items():\n",
    "    print(\"First key:\", key)\n",
    "    print(\"delayed matrix for first story is\", value.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (241, 94251)\n"
     ]
    }
   ],
   "source": [
    "#check if dimensions match with response matrix\n",
    "story_s2 = np.load(f\"{path_to_data}/subject2/adollshouse.npy\")\n",
    "print(\"Shape:\", story_s2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del glove_dict\n",
    "del raw_text\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textlab",
   "language": "python",
   "name": "textlab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
