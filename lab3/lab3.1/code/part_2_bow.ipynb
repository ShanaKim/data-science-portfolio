{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "from preprocessing import make_delayed\n",
    "from preprocessing import downsample_word_vectors\n",
    "\n",
    "# Add the root project folder to sys.path (so ridge_utils becomes importable)\n",
    "project_root = os.path.abspath('..')  # moves up from 'code/'\n",
    "sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "109\n"
     ]
    }
   ],
   "source": [
    "# Load the raw_text.pkl file\n",
    "path_to_data = '/ocean/projects/mth240012p/shared/data'\n",
    "with open(f'{path_to_data}/raw_text.pkl', 'rb') as f:\n",
    "    raw_text = pickle.load(f)\n",
    "\n",
    "print(type(raw_text))\n",
    "print(len(raw_text)) # total 109 stories\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before splitting the stories into train and test, since there are 8 more stories in the raw_text file than in the stories in subject 2 and 3, we exclude these stories since they cannot be used for both test or train. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "101\n"
     ]
    }
   ],
   "source": [
    "all_stories = set(raw_text.keys())\n",
    "subj2_stories = set(os.path.splitext(f)[0] for f in os.listdir(f'{path_to_data}/subject2') if f.endswith('.npy'))\n",
    "subj3_stories = set(os.path.splitext(f)[0] for f in os.listdir(f'{path_to_data}/subject3') if f.endswith('.npy'))\n",
    "print(subj2_stories == subj3_stories) #fortunately, subject 2 and 3 has same stories\n",
    "valid_stories = sorted(list(all_stories & subj2_stories & subj3_stories))\n",
    "print(len(valid_stories)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we split test stories, train stories (3:7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train stories: 70\n",
      "Test stories: 31\n"
     ]
    }
   ],
   "source": [
    "#we will just rename valid_stories as all_stories for the sake of simplicity.\n",
    "all_stories = valid_stories\n",
    "random.seed(42)\n",
    "random.shuffle(all_stories)\n",
    "split_idx = int(0.7 * len(all_stories))  # 70% for training\n",
    "\n",
    "train_stories = all_stories[:split_idx]\n",
    "test_stories = all_stories[split_idx:]\n",
    "\n",
    "print(f\"Train stories: {len(train_stories)}\")\n",
    "print(f\"Test stories: {len(test_stories)}\")\n",
    "train_stories.sort()\n",
    "test_stories.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bag of Words Method**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. For each story in train, we compute a matrix of one-hot vectors for each word in story.\n",
    "\n",
    "Below, we first get a list of all the unique words in all the stories in train_stories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137105\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10360"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#compute and append all words in train_stories\n",
    "allwords = []\n",
    "for story in train_stories:\n",
    "    temp_text = raw_text[story].data\n",
    "    allwords += temp_text\n",
    "\n",
    "print(len(allwords))\n",
    "\n",
    "#compute unique words in train_stories\n",
    "unique_words = list(set(allwords))\n",
    "len(unique_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137105\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10360"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#compute and append all words in train_stories\n",
    "allwords = []\n",
    "for story in train_stories:\n",
    "    temp_text = raw_text[story].data\n",
    "    allwords += temp_text\n",
    "\n",
    "print(len(allwords))\n",
    "\n",
    "#compute unique words in train_stories\n",
    "unique_words = list(set(allwords))\n",
    "len(unique_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make one-hot matrix for each story in training stories\n",
    "\n",
    "word_to_index = {word: idx for idx, word in enumerate(unique_words)} #match each word to index\n",
    "vocab_size = len(unique_words)\n",
    "\n",
    "onehot_matrices = {}  # Storing one-hot matrix per story\n",
    "\n",
    "for story in train_stories:\n",
    "    words = raw_text[story].data\n",
    "    num_words = len(words)\n",
    "    \n",
    "    # Initialize zero matrix\n",
    "    onehot = np.zeros((num_words, vocab_size), dtype=np.int8)\n",
    "    \n",
    "    # Fill in the one-hot vectors\n",
    "    for i, word in enumerate(words):\n",
    "        if word in word_to_index:\n",
    "            j = word_to_index[word]\n",
    "            onehot[i, j] = 1\n",
    "    \n",
    "    onehot_matrices[story] = onehot #store matrix as value and story name as key\n",
    "\n",
    "len(onehot_matrices) #70 keys == 70 train stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First key: adventuresinsayingyes\n",
      "<class 'numpy.ndarray'>\n",
      "original shape of matrix for first story is: (2309, 10360)\n",
      "total length of words in first story is: 2309\n"
     ]
    }
   ],
   "source": [
    "#check if one-hot matrices are calculated well for first training story\n",
    "for key, value in onehot_matrices.items():\n",
    "    print(\"First key:\", key)\n",
    "    print(type(value))\n",
    "    print(\"original shape of matrix for first story is:\", value.shape) #row:number of total words in story\n",
    "    #column: number of unique words in all training stories\n",
    "    break\n",
    "\n",
    "\n",
    "print(\"total length of words in first story is:\", len(raw_text['adventuresinsayingyes'].data)) \n",
    "#number of rows of matrix matches the total length of words in first story"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Downsampling word vectors (one-hot matrices) to number of FMRIs taken for each story\n",
    "Now we call downsample word vectors from the preprocessing.py to get the dimensions to match. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#made dictionary whose keys are name of train story and values are raw_text[story]\n",
    "wordseqs = {story: raw_text[story] for story in train_stories}\n",
    "\n",
    "downsampled_vectors_1 = downsample_word_vectors(\n",
    "    stories=train_stories,\n",
    "    word_vectors=onehot_matrices,\n",
    "    wordseqs=wordseqs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n",
      "First key: adventuresinsayingyes\n",
      "downsampled matrix for first story is (406, 10360)\n",
      "First key: afatherscover\n",
      "downsampled matrix for first story is (327, 10360)\n"
     ]
    }
   ],
   "source": [
    "print(len(downsampled_vectors_1)) #matches the total number of training stories\n",
    "\n",
    "#check the downsampled matrix of first two stories\n",
    "i=0\n",
    "for key, value in downsampled_vectors_1.items():\n",
    "    print(\"First key:\", key)\n",
    "    print(\"downsampled matrix for first story is\", value.shape)\n",
    "    i += 1\n",
    "    if i == 2: break\n",
    "#we can see that row of the matrix shrunk from 1708 to 370"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First key: adventuresinsayingyes\n",
      "trimmed matrix for first story is (391, 10360)\n"
     ]
    }
   ],
   "source": [
    "trimmed_vectors_1 = {} #trimmed matrices for each train story \n",
    "\n",
    "for story, matrix in downsampled_vectors_1.items():\n",
    "    trimmed_matrix = matrix[5:-10]\n",
    "    trimmed_vectors_1[story] = trimmed_matrix\n",
    "\n",
    "# check for first trimmed matrix, which corresponds to story \n",
    "for key, value in trimmed_vectors_1.items():\n",
    "    print(\"First key:\", key)\n",
    "    print(\"trimmed matrix for first story is\", value.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: <class 'numpy.ndarray'>\n",
      "Shape: (391, 94251)\n"
     ]
    }
   ],
   "source": [
    "#We can see that the number of rows of trimmed matrices matches\n",
    "# the number of rows in the FMRI scan \n",
    "itsabox_s2 = np.load(f'{path_to_data}/subject2/adventuresinsayingyes.npy')\n",
    "\n",
    "print(\"Type:\", type(itsabox_s2))\n",
    "print(\"Shape:\", itsabox_s2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Create lagged versions of the features using make_delayed with delays ranging form [1, 4] inclusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First key: adventuresinsayingyes\n",
      "delayed matrix for first story is (391, 41440)\n"
     ]
    }
   ],
   "source": [
    "delayed_vectors_1 = {}\n",
    "\n",
    "for story, trimmed_matrix in trimmed_vectors_1.items():\n",
    "    X_lagged = make_delayed(trimmed_matrix, delays=[1, 2, 3, 4])\n",
    "    delayed_vectors_1[story] = X_lagged\n",
    "\n",
    "# check for first trimmed matrix, which corresponds to story 'penpal'\n",
    "for key, value in delayed_vectors_1.items():\n",
    "    print(\"First key:\", key)\n",
    "    print(\"delayed matrix for first story is\", value.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we make delayed vectors for test stories as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(unique_words) #unique words are from TRAIN STORIES\n",
    "\n",
    "onehot_matrices_test = {}  # Storing one-hot matrix per story\n",
    "\n",
    "for story in test_stories:\n",
    "    words = raw_text[story].data\n",
    "    num_words = len(words)\n",
    "    \n",
    "    # Initialize zero matrix\n",
    "    onehot = np.zeros((num_words, vocab_size), dtype=np.int8)\n",
    "    \n",
    "    # Fill in the one-hot vectors\n",
    "    for i, word in enumerate(words):\n",
    "        if word in word_to_index:\n",
    "            j = word_to_index[word]\n",
    "            onehot[i, j] = 1\n",
    "    \n",
    "    onehot_matrices_test[story] = onehot #store matrix as value and story name as key\n",
    "\n",
    "print(len(onehot_matrices_test)) #30 keys\n",
    "\n",
    "wordseqs2 = {story: raw_text[story] for story in test_stories} #wordseqs for test stories\n",
    "downsampled_vectors_1_test = downsample_word_vectors(\n",
    "    stories=test_stories,\n",
    "    word_vectors=onehot_matrices_test,\n",
    "    wordseqs=wordseqs2\n",
    ")\n",
    "\n",
    "trimmed_vectors_1_test = {} #trimmed matrices for each test story \n",
    "\n",
    "for story, matrix in downsampled_vectors_1_test.items():\n",
    "    trimmed_matrix = matrix[5:-10]\n",
    "    trimmed_vectors_1_test[story] = trimmed_matrix\n",
    "\n",
    "delayed_vectors_1_test = {}\n",
    "\n",
    "for story, trimmed_matrix in trimmed_vectors_1_test.items():\n",
    "    X_lagged = make_delayed(trimmed_matrix, delays=[1, 2, 3, 4])\n",
    "    delayed_vectors_1_test[story] = X_lagged\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "del raw_text\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PART2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ridge_utils.ridge import ridge_corr, ridge_corr_pred\n",
    "import numpy as np\n",
    "import logging\n",
    "from ridge_utils.ridge import ridge_corr\n",
    "from ridge_utils.utils import mult_diag\n",
    "import random\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Subject2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_s2 = '/ocean/projects/mth240012p/shared/data/subject2/'\n",
    "Y_s2_train_dict = {}\n",
    "for story in train_stories:\n",
    "    Y_s2_train_dict[story] = np.load(path_s2 + f\"{story}.npy\")\n",
    "Y_s2_test_dict = {}\n",
    "for story in test_stories:\n",
    "    Y_s2_test_dict[story] = np.load(path_s2 + f\"{story}.npy\")\n",
    "\n",
    "# Stack X to make matrix\n",
    "X_train_full = np.vstack([delayed_vectors_1[s] for s in train_stories]) \n",
    "X_test_full = np.vstack([delayed_vectors_1_test[s] for s in test_stories])    \n",
    "\n",
    "# Stack Y2 to make one matrix\n",
    "Y_train_s2 = np.vstack([Y_s2_train_dict[s] for s in train_stories])\n",
    "Y_test_s2 = np.vstack([Y_s2_test_dict[s] for s in test_stories])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2(1) Fit ridge regression with alphas =1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "ridge_logger = logging.getLogger(\"ridge_corr\")\n",
    "def safe_zscore(v):\n",
    "    mean = v.mean(0)\n",
    "    std = v.std(0)\n",
    "    std[std == 0] = 1 \n",
    "    return (v - mean) / std\n",
    "\n",
    "zs = safe_zscore\n",
    "X_train_full = zs(np.nan_to_num(X_train_full))\n",
    "X_test_full = zs(np.nan_to_num(X_test_full))\n",
    "Y_train_s2 = zs(np.nan_to_num(Y_train_s2))\n",
    "Y_test_s2 = zs(np.nan_to_num(Y_test_s2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(np.isnan(X_train_full).sum()) \n",
    "print(np.isnan(X_test_full).sum())\n",
    "print(np.isnan(Y_train_s2).sum())\n",
    "print(np.isnan(Y_test_s2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ridge_corr:Doing SVD...\n",
      "INFO:ridge_corr:Dropped 70 tiny singular values.. (U is now (24955, 24885))\n",
      "INFO:ridge_corr:Training stimulus has LSV norm: 644.162\n",
      "/jet/home/tyang11/stat-214-lab3/lab3/ridge_utils/ridge.py:8: RuntimeWarning: invalid value encountered in divide\n",
      "  zs = lambda v: (v-v.mean(0))/v.std(0) ## z-score function\n",
      "INFO:ridge_corr:Average difference between actual & assumed Prespvar: -0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean CC for sub2: 0.002158777535075201\n"
     ]
    }
   ],
   "source": [
    "#subject 2\n",
    "num_vox_s2 = 94251\n",
    "corr_s2 = ridge_corr_pred(X_train_full, X_test_full, Y_train_s2, Y_test_s2, valphas= np.ones(num_vox_s2))\n",
    "mean_cc_2 = np.mean(corr_s2[np.isfinite(corr_s2)]) #exclude\n",
    "print(\"Mean CC for sub2:\", mean_cc_2)\n",
    "#We can see that when alphas are poorly set, the mean CC across voxels are very low. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2(1). CV \n",
    "\n",
    "We use 3-fold CV to get best alphas for each voxel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing fold 1/3 for BoW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ridge_corr:Doing SVD...\n",
      "INFO:ridge_corr:Dropped 46 tiny singular values.. (U is now (16401, 16355))\n",
      "INFO:ridge_corr:Training stimulus has LSV norm: 422.999\n",
      "INFO:ridge_corr:Average difference between actual & assumed Prespvar: nan\n",
      "INFO:ridge_corr:Training: alpha=10.000, mean corr=0.00023, max corr=0.05652, over-under(0.20)=0\n",
      "INFO:ridge_corr:Training: alpha=10000.000, mean corr=0.00213, max corr=0.05903, over-under(0.20)=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing fold 2/3 for BoW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ridge_corr:Doing SVD...\n",
      "INFO:ridge_corr:Dropped 47 tiny singular values.. (U is now (16264, 16217))\n",
      "INFO:ridge_corr:Training stimulus has LSV norm: 526.841\n",
      "INFO:ridge_corr:Average difference between actual & assumed Prespvar: 0.000\n",
      "INFO:ridge_corr:Training: alpha=10.000, mean corr=0.00091, max corr=0.05241, over-under(0.20)=0\n",
      "INFO:ridge_corr:Training: alpha=10000.000, mean corr=0.00295, max corr=0.08136, over-under(0.20)=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing fold 3/3 for BoW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ridge_corr:Doing SVD...\n",
      "INFO:ridge_corr:Dropped 47 tiny singular values.. (U is now (17245, 17198))\n",
      "INFO:ridge_corr:Training stimulus has LSV norm: 536.001\n",
      "INFO:ridge_corr:Average difference between actual & assumed Prespvar: 0.000\n",
      "INFO:ridge_corr:Training: alpha=10.000, mean corr=0.00238, max corr=0.06554, over-under(0.20)=0\n",
      "INFO:ridge_corr:Training: alpha=10000.000, mean corr=0.00256, max corr=0.07010, over-under(0.20)=0\n"
     ]
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# 5-fold cross-validation on training stories\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "alphas = np.logspace(1, 4, 2)  #  2 values from 10 to 10000\n",
    "Rcorrs_folds = []  # Store Rcorrs for each fold\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(train_stories)):\n",
    "    print(f\"Processing fold {fold + 1}/3 for BoW\")\n",
    "\n",
    "    # Get the story names for this fold\n",
    "    train = [train_stories[i] for i in train_idx] \n",
    "    val = [train_stories[i] for i in val_idx]     \n",
    "\n",
    "    # Stack X and Y for this fold\n",
    "    X_train = np.vstack([delayed_vectors_1[s] for s in train]) \n",
    "    X_val = np.vstack([delayed_vectors_1[s] for s in val])      \n",
    "    Y_train = np.vstack([Y_s2_train_dict[s] for s in train])    \n",
    "    Y_val = np.vstack([Y_s2_train_dict[s] for s in val])       \n",
    "\n",
    "    # Z-score \n",
    "    X_train = zs(X_train)\n",
    "    X_val = zs(X_val)\n",
    "    Y_train = zs(Y_train)\n",
    "    Y_val = zs(Y_val)\n",
    "\n",
    "    # Run ridge_corr on this fold\n",
    "    Rcorrs = ridge_corr(X_train, X_val, Y_train, Y_val, alphas, use_corr=True)\n",
    "    Rcorrs = np.array(Rcorrs)  # (10, 94251)\n",
    "    Rcorrs_folds.append(Rcorrs)\n",
    "\n",
    "# Average Rcorrs across folds\n",
    "Rcorrs_folds = np.array(Rcorrs_folds)  # (3, 10, 94251)\n",
    "Rcorrs_5foldmean = Rcorrs_folds.mean(axis=0)  # (10, 94251)\n",
    "# Select best alpha for each voxel\n",
    "best_alpha_idx_s2 = np.argmax(Rcorrs_5foldmean, axis=0)  # (94251,)\n",
    "valphas_s2 = alphas[best_alpha_idx_s2]  # (94251,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ridge_corr:Doing SVD...\n",
      "INFO:ridge_corr:Dropped 70 tiny singular values.. (U is now (24955, 24885))\n",
      "INFO:ridge_corr:Training stimulus has LSV norm: 644.162\n",
      "/jet/home/tyang11/stat-214-lab3/lab3/ridge_utils/ridge.py:8: RuntimeWarning: invalid value encountered in divide\n",
      "  zs = lambda v: (v-v.mean(0))/v.std(0) ## z-score function\n",
      "INFO:ridge_corr:Average difference between actual & assumed Prespvar: -0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean test CC for subject 2 (BoW): 0.004206003832647905\n",
      "Median test CC for subject 2 (BoW): 0.0037176974845577293\n",
      "Top 1 percentile test CC for subject 2 (BoW): 0.036475893286999496\n",
      "Top 5 percentile test CC for subject 2 (BoW): 0.024439891364196845\n"
     ]
    }
   ],
   "source": [
    "#given valphas, we calculate test CCs on test stories. \n",
    "final_ccs_s2 = ridge_corr_pred(X_train_full, X_test_full, Y_train_s2, Y_test_s2, valphas_s2)\n",
    "\n",
    "valid_ccs_s2 = final_ccs_s2[np.isfinite(final_ccs_s2)]  # Exclude NaNs/infs\n",
    "mean_cc_s2 = np.mean(valid_ccs_s2)\n",
    "median_cc_s2 = np.median(valid_ccs_s2)\n",
    "top_1_percentile_cc_s2 = np.percentile(valid_ccs_s2, 99)  # Top 1 percentile\n",
    "top_5_percentile_cc_s2 = np.percentile(valid_ccs_s2, 95)  # Top 5 percentile\n",
    "\n",
    "# Print statistics\n",
    "print(f\"Mean test CC for subject 2 (BoW): {mean_cc_s2}\")\n",
    "print(f\"Median test CC for subject 2 (BoW): {median_cc_s2}\")\n",
    "print(f\"Top 1 percentile test CC for subject 2 (BoW): {top_1_percentile_cc_s2}\")\n",
    "print(f\"Top 5 percentile test CC for subject 2 (BoW): {top_5_percentile_cc_s2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "178"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "del Y_s2_train_dict\n",
    "del Y_s2_test_dict\n",
    "del Y_test_s2\n",
    "del Y_train_s2\n",
    "del Rcorrs_folds, Rcorrs_5foldmean\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**subject3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_s3 = '/ocean/projects/mth240012p/shared/data/subject3/'\n",
    "Y_s3_train_dict = {}\n",
    "for story in train_stories:\n",
    "    Y_s3_train_dict[story] = np.load(path_s3 + f\"{story}.npy\")\n",
    "Y_s3_test_dict = {}\n",
    "for story in test_stories:\n",
    "    Y_s3_test_dict[story] = np.load(path_s3 + f\"{story}.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stack Y3 to make one matrix\n",
    "Y_train_s3 = np.vstack([Y_s3_train_dict[s] for s in train_stories])\n",
    "Y_test_s3 = np.vstack([Y_s3_test_dict[s] for s in test_stories])\n",
    "Y_train_s3 = zs(np.nan_to_num(Y_train_s3))\n",
    "Y_test_s3 = zs(np.nan_to_num(Y_test_s3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2(1). fit ridge for alpha = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ridge_corr:Doing SVD...\n",
      "INFO:ridge_corr:Dropped 70 tiny singular values.. (U is now (24955, 24885))\n",
      "INFO:ridge_corr:Training stimulus has LSV norm: 644.162\n",
      "INFO:ridge_corr:Average difference between actual & assumed Prespvar: -0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean CC for sub3: 0.0017076314413972662\n"
     ]
    }
   ],
   "source": [
    "#subject3\n",
    "num_vox_s3 = 95556\n",
    "corr_s3 = ridge_corr_pred(X_train_full, X_test_full, Y_train_s3, Y_test_s3, valphas= np.ones(num_vox_s3))\n",
    "mean_cc_3 = np.mean(corr_s3[np.isfinite(corr_s3)]) #exclude\n",
    "print(\"Mean CC for sub3:\", mean_cc_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2(2). CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing fold 1/3 for BoW (subject 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ridge_corr:Doing SVD...\n",
      "INFO:ridge_corr:Dropped 46 tiny singular values.. (U is now (16401, 16355))\n",
      "INFO:ridge_corr:Training stimulus has LSV norm: 422.999\n",
      "INFO:ridge_corr:Average difference between actual & assumed Prespvar: -0.000\n",
      "INFO:ridge_corr:Training: alpha=10.000, mean corr=0.00152, max corr=0.05325, over-under(0.20)=0\n",
      "INFO:ridge_corr:Training: alpha=10000.000, mean corr=0.00518, max corr=0.07571, over-under(0.20)=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing fold 2/3 for BoW (subject 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ridge_corr:Doing SVD...\n",
      "INFO:ridge_corr:Dropped 47 tiny singular values.. (U is now (16264, 16217))\n",
      "INFO:ridge_corr:Training stimulus has LSV norm: 526.841\n",
      "INFO:ridge_corr:Average difference between actual & assumed Prespvar: 0.000\n",
      "INFO:ridge_corr:Training: alpha=10.000, mean corr=0.00369, max corr=0.08274, over-under(0.20)=0\n",
      "INFO:ridge_corr:Training: alpha=10000.000, mean corr=0.00565, max corr=0.10215, over-under(0.20)=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing fold 3/3 for BoW (subject 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ridge_corr:Doing SVD...\n",
      "INFO:ridge_corr:Dropped 47 tiny singular values.. (U is now (17245, 17198))\n",
      "INFO:ridge_corr:Training stimulus has LSV norm: 536.001\n",
      "INFO:ridge_corr:Average difference between actual & assumed Prespvar: 0.000\n",
      "INFO:ridge_corr:Training: alpha=10.000, mean corr=0.00294, max corr=0.08075, over-under(0.20)=0\n",
      "INFO:ridge_corr:Training: alpha=10000.000, mean corr=0.00636, max corr=0.10034, over-under(0.20)=0\n"
     ]
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# 5-fold cross-validation on training stories\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "alphas = np.logspace(1, 4, 2) \n",
    "Rcorrs_folds = []  # Reset for subject 3\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(train_stories)):\n",
    "    print(f\"Processing fold {fold + 1}/3 for BoW (subject 3)\")\n",
    "\n",
    "    train = [train_stories[i] for i in train_idx] \n",
    "    val = [train_stories[i] for i in val_idx]\n",
    "\n",
    "    # Stack X and Y for this fold\n",
    "    X_train = np.vstack([delayed_vectors_1[s] for s in train]) \n",
    "    X_val = np.vstack([delayed_vectors_1[s] for s in val])      \n",
    "    Y_train = np.vstack([Y_s3_train_dict[s] for s in train])    \n",
    "    Y_val = np.vstack([Y_s3_train_dict[s] for s in val])       \n",
    "\n",
    "    # Z-score \n",
    "    X_train = zs(X_train)\n",
    "    X_val = zs(X_val)\n",
    "    Y_train = zs(Y_train)\n",
    "    Y_val = zs(Y_val)\n",
    "\n",
    "    # Run ridge_corr on this fold\n",
    "    Rcorrs = ridge_corr(X_train, X_val, Y_train, Y_val, alphas, use_corr=True)\n",
    "    Rcorrs = np.array(Rcorrs)  # (10, 94251)\n",
    "    Rcorrs_folds.append(Rcorrs)\n",
    "\n",
    "# Average Rcorrs across folds\n",
    "Rcorrs_folds = np.array(Rcorrs_folds)  # (3, 10, 95556)\n",
    "Rcorrs_5foldmean = Rcorrs_folds.mean(axis=0)  # (10, 95556)\n",
    "\n",
    "# Select best alpha for each voxel\n",
    "best_alpha_idx_s3 = np.argmax(Rcorrs_5foldmean, axis=0)  # (95556,)\n",
    "valphas_s3 = alphas[best_alpha_idx_s3]  # (95556,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ridge_corr:Doing SVD...\n",
      "INFO:ridge_corr:Dropped 70 tiny singular values.. (U is now (24955, 24885))\n",
      "INFO:ridge_corr:Training stimulus has LSV norm: 644.162\n",
      "INFO:ridge_corr:Average difference between actual & assumed Prespvar: -0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean test CC for subject 3 (BoW): 0.004901137088664349\n",
      "Median test CC for subject 3 (BoW): 0.003983045173689538\n",
      "Top 1 percentile test CC for subject 3 (BoW): 0.04508222720607162\n",
      "Top 5 percentile test CC for subject 3 (BoW): 0.02705549544698708\n"
     ]
    }
   ],
   "source": [
    "# Given valphas, calculate test CCs on test stories\n",
    "final_ccs_s3 = ridge_corr_pred(X_train_full, X_test_full, Y_train_s3, Y_test_s3, valphas_s3, use_corr=True)\n",
    "\n",
    "# Compute statistics for subject 3\n",
    "valid_ccs_s3 = final_ccs_s3[np.isfinite(final_ccs_s3)]  # Exclude NaNs/infs\n",
    "mean_cc_s3 = np.mean(valid_ccs_s3)\n",
    "median_cc_s3 = np.median(valid_ccs_s3)\n",
    "top_1_percentile_cc_s3 = np.percentile(valid_ccs_s3, 99)  # Top 1 percentile\n",
    "top_5_percentile_cc_s3 = np.percentile(valid_ccs_s3, 95)  # Top 5 percentile\n",
    "\n",
    "# Print statistics\n",
    "print(f\"Mean test CC for subject 3 (BoW): {mean_cc_s3}\")\n",
    "print(f\"Median test CC for subject 3 (BoW): {median_cc_s3}\")\n",
    "print(f\"Top 1 percentile test CC for subject 3 (BoW): {top_1_percentile_cc_s3}\")\n",
    "print(f\"Top 5 percentile test CC for subject 3 (BoW): {top_5_percentile_cc_s3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
