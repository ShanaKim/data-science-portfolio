Soohyun Kim:
Started off lab 3.3 and successfully implemented the BERT-base-uncased model to generate embeddings and run ridge.
Organized team meetings and set internal deadlines. 
Wrote Intro, first few sections for the report, and proofread report. Pushed work to git.

Tianrui Yang:
Implement the core code of part 2: interpret the model using SHAP and LIME and investigate the word importances for a simple voxel.
Actively organized and participated in the group discussions, and started the skeleton of the report. 
Wrote Section 3.1, 3.2, the first half of 3.3 and the Academic honesty part of the report.

Yujian Zhou:
I implemented fine-tuning of the BERT model through continued pre-training and adapted it using LoRA. 
I also carried out downstream tasks such as generating embeddings and fitting a ridge regression model like previous labs. Then I compared model performance across various embedding methods. 
Besides, I also wrote corresponding sections of our final report.

Haavard Fossdall:
For Lab 3.3, I worked on Part 2 and ran the code on Bridges2. 
I extended the existing SHAP and LIME code to support multiple voxels and test stories, wrapping it into a flexible function. 
I also wrote on sections 3.3 and 3.4, interpreting the plots, and reviewed the report for grammar and clarity.